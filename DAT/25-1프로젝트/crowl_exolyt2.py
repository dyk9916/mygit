import pandas as pd
import time
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

# ğŸ“Œ ì—‘ì…€ì—ì„œ Username ë¦¬ìŠ¤íŠ¸ ì½ê¸°
def get_usernames():
    excel_path = r'D:/ê¹€ë™ì˜/11_Github/mygit-1/DAT/25-1í”„ë¡œì íŠ¸/ì†Œì…œí•¸ë“¤.xlsx'
    df = pd.read_excel(excel_path)
    return df['Username'].dropna().apply(lambda x: x.lstrip('@')).tolist()

# ğŸ“Œ í†µê³„ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ (Avg. Views, Avg. Comments, Total Video Shares)
def extract_stats(soup):
    stat_labels = {
        "Avg. Views": "Average views",
        "Avg. Comments": "Average comments",
        "Total Video Shares": "Video shares"
    }

    stats = {k: None for k in stat_labels.keys()}

    for key, label_text in stat_labels.items():
        label_elem = soup.find(lambda tag: tag.name in ["div", "span"] and label_text.lower() in tag.get_text(strip=True).lower())
        if label_elem:
            parent = label_elem.find_parent()
            if parent:
                sibling_texts = [el.get_text(strip=True) for el in parent.find_all(["div", "span"]) if el != label_elem]
                for text in sibling_texts:
                    if any(c.isdigit() for c in text):
                        stats[key] = text
                        break

    return stats

# ğŸ“Œ ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl():
    usernames = get_usernames()
    usernames = usernames[:1]  # âœ… í…ŒìŠ¤íŠ¸ìš©: ì²« ë²ˆì§¸ ì‚¬ìš©ìë§Œ í¬ë¡¤ë§
    print("ì‚¬ìš©í•  usernames:", usernames)

    result_df = pd.DataFrame(columns=["username", "avg_views", "avg_comments", "video_promotions"])

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(storage_state="exolyt_login_state.json")  # ë¡œê·¸ì¸ ì„¸ì…˜ í•„ìš”
        page = context.new_page()

        for username in usernames:
            print(f"\nğŸš€ ìˆ˜ì§‘ ì‹œì‘: {username}")
            try:
                page.goto(f"https://exolyt.com/user/tiktok/{username}")
                time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

                soup = BeautifulSoup(page.content(), "html.parser")
                stats = extract_stats(soup)

                result_df = pd.concat([result_df, pd.DataFrame([{
                    "username": username,
                    "avg_views": stats["Avg. Views"],
                    "avg_comments": stats["Avg. Comments"],
                    "video_promotions": stats["Total Video Shares"]
                }])], ignore_index=True)

                print(f"âœ… {username} - ìˆ˜ì§‘ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ {username} - ì˜¤ë¥˜ ë°œìƒ: {e}")

        browser.close()
        result_df.to_csv("í¬ë¡¤ë§_ê²°ê³¼.csv", index=False, encoding='utf-8-sig')
        print("\nğŸ“„ ìµœì¢… ê²°ê³¼:")
        print(result_df)

# ğŸ“Œ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    crawl()
