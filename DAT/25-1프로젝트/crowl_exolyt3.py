import pandas as pd
import time
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

# âœ… ì—‘ì…€ì—ì„œ Username ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
def get_usernames():
    excel_path = r'D:/ê¹€ë™ì˜/11_Github/mygit-1/DAT/25-1í”„ë¡œì íŠ¸/ì†Œì…œí•¸ë“¤.xlsx'
    df = pd.read_excel(excel_path)
    return df['Username'].dropna().apply(lambda x: x.lstrip('@')).tolist()

# âœ… í†µê³„ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ (Likes, Total Video Sharesë¥¼ ë¼ë²¨ ê¸°ì¤€ìœ¼ë¡œ ì •í™•í•˜ê²Œ íƒìƒ‰)
def extract_stats(soup):
    stats = {"Likes": None, "Total Video Shares": None}  # ìˆ˜ì§‘í•˜ë ¤ëŠ” ë¼ë²¨ë“¤

    for label in stats.keys():
        # ğŸ¯ 'Likes' ë˜ëŠ” 'Total Video Shares' í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ div/span ì°¾ê¸°
        label_elem = soup.find(
            lambda tag: tag.name in ["div", "span"] and label.lower() in tag.get_text(strip=True).lower()
        )
        if label_elem:
            # ğŸ“Œ í•´ë‹¹ ë¼ë²¨ì´ ìˆëŠ” íƒœê·¸ì˜ ë¶€ëª¨ ìš”ì†Œ íƒìƒ‰
            parent = label_elem.find_parent()
            if parent:
                # ğŸ‘€ ë¶€ëª¨ ë‚´ë¶€ì—ì„œ í˜•ì œ ìš”ì†Œë“¤ ì¤‘ ìˆ«ìë¥¼ í¬í•¨í•œ ìš”ì†Œë§Œ ì¶”ì¶œ
                siblings = parent.find_all(["div", "span"])
                for sib in siblings:
                    text = sib.get_text(strip=True)
                    if sib != label_elem and any(c.isdigit() for c in text):
                        stats[label] = text  # ğŸ¯ ì‹¤ì œ ìˆ˜ì¹˜ë¥¼ ì—¬ê¸°ì— ì €ì¥
                        break

    return stats  # {'Likes': '1.2M', 'Total Video Shares': '5.3K'} í˜•íƒœë¡œ ë°˜í™˜

# âœ… ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl():
    usernames = get_usernames()
    print("ì‚¬ìš©í•  usernames:", usernames)

    # ğŸ“„ ê²°ê³¼ ì €ì¥ìš© ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    result_df = pd.DataFrame(columns=["username", "likes", "total_video_shares"])

    with sync_playwright() as p:
        # ğŸ” í¬ë¡¬ ë¸Œë¼ìš°ì € ì—´ê¸° + ë¡œê·¸ì¸ ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ê¸°
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(storage_state="exolyt_login_state.json")
        page = context.new_page()

        for username in usernames:
            print(f"\nğŸš€ ìˆ˜ì§‘ ì‹œì‘: {username}")
            try:
                # ğŸŒ TikTok ê³„ì • ë¶„ì„ í˜ì´ì§€ ì´ë™
                page.goto(f"https://exolyt.com/user/tiktok/{username}")
                time.sleep(5)  # â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

                soup = BeautifulSoup(page.content(), "html.parser")  # ğŸ§ª HTML ë¶„ì„ ì¤€ë¹„
                stats = extract_stats(soup)  # ğŸ“¥ í†µê³„ ì •ë³´ ìˆ˜ì§‘

                # ğŸ“Š ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
                result_df = pd.concat([result_df, pd.DataFrame([{
                    "username": username,
                    "likes": stats["Likes"],
                    "total_video_shares": stats["Total Video Shares"]
                }])], ignore_index=True)

                print(f"âœ… {username} - ìˆ˜ì§‘ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ {username} - ì˜¤ë¥˜ ë°œìƒ: {e}")

        browser.close()  # ğŸ’» ë¸Œë¼ìš°ì € ì¢…ë£Œ

    # ğŸ’¾ CSVë¡œ ì €ì¥
    result_df.to_csv("í¬ë¡¤ë§_ê²°ê³¼.csv", index=False, encoding="utf-8-sig")
    print("\nğŸ“„ ìµœì¢… ê²°ê³¼:")
    print(result_df)

# âœ… í”„ë¡œê·¸ë¨ ì‹œì‘ ì§€ì 
if __name__ == "__main__":
    crawl()
